{'quiz': [{'question_id': 1, 'question': 'What is the core mechanism that the Transformer architecture primarily relies on?', 'options': ['Recurrent layers', 'Convolutional layers', 'Attention mechanisms', 'Dense layers'], 'correct_answer': 'Attention mechanisms', 'explanation': "The Transformer replaces recurrent and convolutional layers entirely with attention mechanisms, as stated in the original paper's title and abstract.", 'related_chunks': ['0:1']}, {'question_id': 2, 'question': 'Which components make up the encoder and decoder stacks in the Transformer?', 'options': ['RNN and CNN layers', 'Self-attention and feed-forward layers', 'LSTM and GRU layers', 'Pooling and normalization layers'], 'correct_answer': 'Self-attention and feed-forward layers', 'explanation': 'The encoder and decoder use stacked self-attention and point-wise fully connected feed-forward layers, as described in the model architecture section.', 'related_chunks': ['1:5']}, {'question_id': 3, 'question': 'What key advantage does the Transformer have over RNN-based models for machine translation?', 'options': ['Lower memory usage', 'Better sequential processing', 'Increased parallelizability', 'Simpler loss functions'], 'correct_answer': 'Increased parallelizability', 'explanation': "The Transformer's attention-based architecture allows for more parallelization compared to sequential RNNs, as highlighted in the abstract.", 'related_chunks': ['0:1']}, {'question_id': 4, 'question': 'Which positional encoding method did the base Transformer model use?', 'options': ['Learned embeddings', 'Sinusoidal functions', 'Random initialization', 'One-hot encoding'], 'correct_answer': 'Sinusoidal functions', 'explanation': 'The base model used sinusoidal positional encodings, though learned embeddings produced similar results according to ablation studies.', 'related_chunks': ['8:2']}, {'question_id': 5, 'question': 'What is the third sub-layer added to each decoder layer that is not present in the encoder?', 'options': ['Self-attention layer', 'Feed-forward layer', 'Encoder-decoder attention layer', 'Normalization layer'], 'correct_answer': 'Encoder-decoder attention layer', 'explanation': "The decoder includes a third sub-layer for multi-head attention over the encoder's output, unlike the encoder layers.", 'related_chunks': ['2:1']}, {'question_id': 6, 'question': 'Why does the decoder use masked self-attention?', 'options': ['To reduce computation', 'To prevent attending to future positions', 'To improve gradient flow', 'To increase model capacity'], 'correct_answer': 'To prevent attending to future positions', 'explanation': 'Masking ensures the decoder can only attend to earlier positions in the output sequence during training.', 'related_chunks': ['2:1']}, {'question_id': 7, 'question': 'What was observed when reducing the attention key size (d_k) in ablation studies?', 'options': ['Improved model quality', 'Reduced training time', 'Decreased model quality', 'No significant effect'], 'correct_answer': 'Decreased model quality', 'explanation': 'Smaller attention key sizes hurt performance, suggesting the importance of sufficient dimensionality for attention compatibility.', 'related_chunks': ['8:2']}, {'question_id': 8, 'question': 'What BLEU score did the Transformer achieve on the WMT 2014 English-to-French translation task?', 'options': ['28.4', '35.2', '41.0', '45.6'], 'correct_answer': '41.0', 'explanation': 'The model achieved a state-of-the-art 41.0 BLEU score on this task, as reported in the experimental results.', 'related_chunks': ['0:2']}, {'question_id': 9, 'question': 'What is the purpose of residual connections followed by layer normalization in each sub-layer?', 'options': ['To increase model depth', 'To stabilize training', 'To reduce parameters', 'To enable recurrence'], 'correct_answer': 'To stabilize training', 'explanation': 'Residual connections and layer normalization help maintain stable gradients and improve training efficiency.', 'related_chunks': ['2:1']}, {'question_id': 10, 'question': 'What is the output dimension of all sub-layers and embedding layers in the base Transformer model?', 'options': ['256', '512', '1024', '2048'], 'correct_answer': '512', 'explanation': 'The base model uses d_model = 512 for all sub-layer and embedding outputs to enable residual connections.', 'related_chunks': ['2:1']}]}